{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas\n",
    "\n",
    "- Create classes to represent Data, Metadata, and Dataset? (dataset being the combination of the two, with fns which link them; ie. for cross-checking the type defined in meta is what the data type actually is)\n",
    "- For Meta, can be read in different formats, and then exported in different formats\n",
    "  - For dataset, meta must be a particular format\n",
    "\n",
    "- Filter out rows with all nulls"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Initial import and formatting of SPSS data.\n",
    "output-file: read.html\n",
    "title: read\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev import nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from polars.testing import assert_frame_equal\n",
    "import numpy as np\n",
    "import pyreadstat\n",
    "import pyspssio\n",
    "from pathlib import Path\n",
    "from typing import Optional, Any\n",
    "\n",
    "from fastcore.utils import *\n",
    "from fastcore.test import *\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from pydantic.dataclasses import dataclass\n",
    "# from pandera import ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA = Path(\"../data/raw\")\n",
    "PROCESSED_DATA = Path(\"../data/processed\")\n",
    "df, meta = pyspssio.read_sav(RAW_DATA/\"G214_PQ.sav\")\n",
    "df = pl.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# TODO: convert to Pydantic BaseModel (ensure that variable types are one of \"nominal\", \"scale\" or \"ordinal\")\n",
    "@dataclass\n",
    "class Metadata:\n",
    "    variable_basename: str\n",
    "    label: str\n",
    "    field_values: dict[int, str]\n",
    "    field_type: str\n",
    "    field_width: int\n",
    "    decimals: int\n",
    "    variable_type: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test this works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metadata(variable_basename='PN17', label='Ever had back pain', field_values={-99: 'Missing', 0: 'No', 1: 'Yes'}, field_type='Numeric', field_width=3, decimals=0, variable_type='Nominal')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Metadata(\n",
    "    variable_basename = \"PN17\",\n",
    "    label = \"Ever had back pain\",\n",
    "    field_values = {-99: \"Missing\", 0: \"No\", 1: \"Yes\"},\n",
    "    field_type = \"Numeric\",\n",
    "    field_width = 3,\n",
    "    decimals =  0,\n",
    "    variable_type = \"Nominal\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def unpack_variable_types(input_dict):\n",
    "    \"...\"\n",
    "    field_type = {}\n",
    "    decimals = {}\n",
    "    \n",
    "    for key, value in input_dict.items():\n",
    "        if value.startswith('F'):\n",
    "            field_type[key] = 'Numeric'\n",
    "            dec = value.split('.')[1]\n",
    "            decimals[key] = int(dec) if dec != '0' else 0\n",
    "        elif value.startswith('DATE'):\n",
    "            field_type[key] = 'Date'\n",
    "            decimals[key] = 0\n",
    "        elif value.startswith('A'):\n",
    "            field_type[key] = 'String'\n",
    "            decimals[key] = 0\n",
    "    \n",
    "    return field_type, decimals\n",
    "\n",
    "def reformat_metadata(m: pyreadstat.metadata_container, # metadata from pyreadstat\n",
    "                     ) -> dict[dict[str, Any]]:\n",
    "    \"Reformat metadata into a more readable and consistent format\"\n",
    "    field_type, decimals = unpack_variable_types(m.original_variable_types)\n",
    "    metadata = {\n",
    "        \"Label\": m.column_names_to_labels,\n",
    "        \"Field Type\": field_type,\n",
    "        \"Field Width\": m.variable_display_width,\n",
    "        \"Decimals\": decimals,\n",
    "        \"Variable Type\": m.variable_measure,\n",
    "        \"Field Values\": m.variable_value_labels\n",
    "    }\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: add Data and Metadata classes, and nest them in the Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def read_sav(data_dir: str|Path, \n",
    "             file: str, \n",
    "             cols: Optional[list[str]] = None\n",
    "             ) -> pl.LazyFrame:\n",
    "    df, meta = pyreadstat.read_sav(f\"{data_dir}/{file}\", usecols=cols)\n",
    "    df = pl.from_pandas(df).lazy()\n",
    "    return df, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@dataclass\n",
    "class Dataset:\n",
    "    file: str\n",
    "    data_dir: str | Path\n",
    "    prefix: Optional[str] = None\n",
    "    variables: Optional[list[str]] = None\n",
    "\n",
    "    def load_data(self) -> tuple[pl.LazyFrame, dict[dict[str, Any]]]:\n",
    "        \"Output data and metadata.\"\n",
    "        df, meta = read_sav(self.data_dir, self.file, self.variables)\n",
    "        meta = reformat_metadata(meta)\n",
    "        return df, meta\n",
    "    \n",
    "        # def strip_prefix(self, df: pl.LazyFrame) -> pl.DataFrame:\n",
    "    #     stripped_columns = {col: col.replace(self.prefix, \"\") for col in self.variables}\n",
    "    #     df = df.rename(stripped_columns)\n",
    "    #     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, meta = Dataset(\"G214_PQ.sav\", RAW_DATA).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    Dataset(\"G214_PQ.sav\", RAW_DATA, \"G214_PQ_\", [\"ID\", \"G214_PQ_PN17\", \"G214_PQ_PN25\", \"G214_PQ_PN34\", \"G214_PQ_PN35\", \"G214_PQ_PN36\"]),\n",
    "    Dataset(\"G214_SQ.sav\", RAW_DATA, \"G214_SQ_\", [\"ID\", \"G214_SQ_PN17\", \"G214_SQ_PN25\", \"G214_SQ_PN34\", \"G214_SQ_PN35\", \"G214_SQ_PN36\"]),\n",
    "    Dataset(\"G217_PQ.sav\", RAW_DATA, \"G217_PQ_\", [\"ID\", \"G217_PQ_PN17\", \"G217_PQ_PN25\", \"G217_PQ_PN34\", \"G217_PQ_PN35\", \"G217_PQ_PN36\", \"G217_PQ_PN38\", \"G217_PQ_PN9\"]),\n",
    "    Dataset(\"G217_SQ.sav\", RAW_DATA, \"G217_SQ_\", [\"ID\", \"G217_SQ_PN17\", \"G217_SQ_PN25\", \"G217_SQ_PN34\", \"G217_SQ_PN35\", \"G217_SQ_PN36\", \"G217_SQ_PN38\", \"G217_SQ_PN9\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def read_and_filter_data(datasets: list[Dataset]\n",
    "                         ) -> list[pl.LazyFrame]:\n",
    "    \"\"\"\n",
    "    Take a list of `Dataset`s and return a list of the respective dataframes \n",
    "    and metadata for each dataset, filtered for the given columns.\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "    metadata = []\n",
    "    for ds in datasets:\n",
    "        df, meta = ds.load_data()\n",
    "        dataframes.append(df)\n",
    "        metadata.append(meta)\n",
    "    return dataframes, metadata\n",
    "\n",
    "def combine_dataframes(dataframes: list[pl.LazyFrame]\n",
    "                       ) -> pl.LazyFrame:\n",
    "    \"Take a list of dataframes and return a single, combined dataframe.\"\n",
    "    combined_df = dataframes[0]\n",
    "    for df in dataframes[1:]:\n",
    "        combined_df = combined_df.join(df, on=\"ID\", how=\"full\", coalesce=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes, metadata = read_and_filter_data(datasets)\n",
    "df = combine_dataframes(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def merge_dictionaries(dicts: list[dict[str, Any]]\n",
    "                       ) -> dict[str, Any]:\n",
    "    \"Merge a series of nested dictionaries.\"\n",
    "    merged_dict = defaultdict(dict)\n",
    "    for d in dicts:\n",
    "        for key, nested_dict in d.items():\n",
    "            for nested_key, value in nested_dict.items():\n",
    "                if nested_key not in merged_dict[key]:\n",
    "                    merged_dict[key][nested_key] = value\n",
    "                elif isinstance(value, dict):\n",
    "                    merged_dict[key][nested_key].update(value)\n",
    "    return dict(merged_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_merged = merge_dictionaries(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def convert_metadata_list_to_dict(metadata: list[Metadata], # list of Metadata objects\n",
    "                                  p: str # Prefix for dataset\n",
    "                                  ) -> dict[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Take a list of Metadata objects and convert them into the SPSS format\n",
    "    with parameters as parents and variables as children, in nested dictionaries.\n",
    "    \"\"\"\n",
    "    converted_metadata = {\n",
    "        \"Label\": {p + m.variable_basename: m.label for m in metadata},\n",
    "        \"Field Values\": {p + m.variable_basename: m.field_values for m in metadata},\n",
    "        \"Field Type\": {p + m.variable_basename: m.field_type for m in metadata},\n",
    "        \"Field Width\": {p + m.variable_basename: m.field_width for m in metadata},\n",
    "        \"Decimals\": {p + m.variable_basename: m.decimals for m in metadata},\n",
    "        \"Variable Type\": {p + m.variable_basename: m.variable_type for m in metadata},\n",
    "    }\n",
    "    return converted_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def pack_variable_types(m: dict[dict[str, Any]] # metadata in nested dictionary format\n",
    "                        ) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Convert metadata parameters related to variable format \n",
    "    into an appropriate schema for pyreadstat.\n",
    "    \"\"\"\n",
    "    field_type = m[\"Field Type\"]\n",
    "    field_width = m[\"Field Width\"]\n",
    "    decimals = m[\"Decimals\"]\n",
    "    \n",
    "    # Verify that variables are identical across each metadata parameter\n",
    "    assert field_type.keys() == field_width.keys() == decimals.keys(), \"Not all variables match.\"\n",
    "\n",
    "    result = _pack_variable_types(field_type, field_width, decimals)\n",
    "    return result\n",
    "    \n",
    "def _pack_variable_types(field_type: dict[str, str],\n",
    "                         field_width: dict[str, int],\n",
    "                         decimals: dict[str, int]\n",
    "                         ) -> dict[str, str]:\n",
    "    \"\"\"Private function to perform the logic for `pack_variable_types`.\"\"\"\n",
    "    d = {}\n",
    "\n",
    "    field_type_map = {\n",
    "        \"Numeric\": \"F\",\n",
    "        \"String\": \"A\",\n",
    "        \"Date\": \"DATE\"\n",
    "    }\n",
    "\n",
    "    for key in field_type:\n",
    "        f_type = field_type_map[field_type[key]]\n",
    "        f_width = field_width[key]\n",
    "        dec = decimals[key]\n",
    "        \n",
    "        if field_type[key] == \"Numeric\" and dec > 0:\n",
    "            d[key] = f\"{f_type}{f_width}.{dec}\"\n",
    "        else:\n",
    "            d[key] = f\"{f_type}{f_width}\"\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = {\n",
    "    'G214_PQ_PN17': 'Numeric',\n",
    "    'G214_PQ_DNWN': 'Date',\n",
    "    'G214_PQ_HOC1': 'String'\n",
    "}\n",
    "\n",
    "fw = {\n",
    "    'G214_PQ_PN17': 3,\n",
    "    'G214_PQ_DNWN': 8,\n",
    "    'G214_PQ_HOC1': 8\n",
    "}\n",
    "\n",
    "d = {\n",
    "    'G214_PQ_PN17': 0,\n",
    "    'G214_PQ_DNWN': 0,\n",
    "    'G214_PQ_HOC1': 0\n",
    "}\n",
    "\n",
    "m = {\n",
    "    \"Field Type\": ft,\n",
    "    \"Field Width\": fw,\n",
    "    \"Decimals\": d\n",
    "}\n",
    "\n",
    "expected_output = {\n",
    "    'G214_PQ_PN17': \"F3\",\n",
    "    'G214_PQ_DNWN': \"DATE8\",\n",
    "    'G214_PQ_HOC1': \"A8\"\n",
    "}\n",
    "\n",
    "test_eq(pack_variable_types(m), expected_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# TODO: could be actual Data and Metadata class/types\n",
    "def write_sav(dst_path: str|Path, # path to save output file\n",
    "              df: pl.LazyFrame, # raw data\n",
    "              metadata: dict[str, dict[str, Any]] # corresponding metadata\n",
    "              ) -> None:\n",
    "    \"\"\"Save dataset to SPSS using `pyreadstat` library.\"\"\"\n",
    "    # Convert \n",
    "    df = df.collect().to_pandas()\n",
    "\n",
    "    pyreadstat.write_sav(\n",
    "        df, \n",
    "        dst_path,\n",
    "        column_labels=metadata[\"Label\"],\n",
    "        variable_value_labels=metadata[\"Field Values\"],\n",
    "        variable_display_width=metadata[\"Field Width\"],\n",
    "        variable_measure=metadata[\"Variable Type\"], # TODO: convert to all lowercase\n",
    "        variable_format=pack_variable_types(metadata)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the custom SPSS writing function makes no unintended changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "G214_PQ = Dataset(\"G214_PQ.sav\", RAW_DATA)\n",
    "df1, meta1 = G214_PQ.load_data()\n",
    "\n",
    "# Write the dataset unchanged to a new file\n",
    "write_sav(PROCESSED_DATA/\"G214_PQ.sav\", df1, meta1)\n",
    "\n",
    "# Read that newly saved file\n",
    "output = Dataset(\"G214_PQ.sav\", PROCESSED_DATA)\n",
    "df2, meta2 = output.load_data()\n",
    "\n",
    "# Compare for both data and metadata to verify no unintended changes have been introduced\n",
    "assert_frame_equal(df1, df2)\n",
    "test_eq(meta1, meta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
